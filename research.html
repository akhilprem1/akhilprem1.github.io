<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Research – Akhil Premkumar</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="css/style.css" />
</head>

<body>

<header class="hero">
  <div class="hero-inner">
    <div class="hero-strip">
      <div class="hero-text">
        <h1>Akhil Premkumar</h1>
        <p>Physics × Generative AI</p>
      </div>
    </div>
  </div>
</header>

<nav class="top-nav">
  <div class="top-nav-inner">
    <a href="index.html">Home</a>
    <a href="about.html">About</a>
    <a href="research.html" class="active">Research</a>
    <!-- <a href="teaching.html">Teaching</a> -->
    <a href="publications.html">Publications</a>
  </div>
</nav>

<main>

  <!-- Research Interests (Your Provided Text) ---------------------------- -->
  <section id="research-interests">
    <h3>Research Interests</h3>
    
    <div class="item">
    <div class="item-title">Diffusion Models</div>
    <div class="item-meta-dark">
        These are a class of generative algorithms, often used for image and video models.
        Diffusion models serve as a natural bridge between machine learning, thermodynamics/statistical mechanics, information theory,
        and optimal control theory, just to name a few. I've used these connections to:
        <ul class="sub-questions">
        <li>Quantify the information is stored in a neural network,</li>
        <li>Understand how the model learns correlations within the data, and across data sources,</li>
        <li>Connect generative modeling with optimal betting strategies.</li>
        </ul>
    </div>
    </div>

    <div class="item">
    <div class="item-title">Reinforcement Learning</div>
    <div class="item-meta-dark">
      I've become interested in RL lately, with an eye towards solving the entropy collapse problem in RL algorithms.
      When the policy becomes prematurely over-confident, it commits to a narrow set of actions and fails to discover better strategies.
      This leads to brittle behavior, poor generalization, and instability during training. Preventing this collapse is essential for
      unlocking more robust, adaptive, and genuinely creative AI agents.
    </div>
    </div>
  </section>



  <!-- Past Research ------------------------------------------------------ -->
  <section id="past-work">
    <h3>Past Research</h3>

    <div class="item">
      <div class="item-title">Quantum Fields in de Sitter Spacetime</div>
      <div class="item-meta">
        The study of inflationary cosmology involves the interplay between quantum mechanics with gravity in the backdrop of an expanding spacetime, called de Sitter.
        Quantum field theoretic calculations in such a setting give rise to unique divergences that are absent in regular particle physics. I pioneered new techniques for
        carrying out such computations analytically, while taming and interpreting these infinities.
      </div>
    </div>

    <div class="item">
      <div class="item-title">Stochastic Inflation and Large Deviations</div>
      <div class="item-meta">
        In a certain long wavelength limit, the evolution of inflationary fields in de Sitter follow a diffusion equation, with randomness sourced by quantum uncertainty.
        I studied higher-order corrections to this picture, integrating ideas from effective field theory, large deviation theory, and stochastic processes.
        We showed that, due to the exponential nature of the expanding spacetime, rare fluctuations of the field have a surprisingly consequential influence on
        inflationary dynamics. Many of the ideas I explored here would come to heavily influence my research into diffusion models.
      </div>
    </div>
  </section>

</main>

<footer>
  © Akhil Premkumar, 2025
</footer>

</body>
</html>